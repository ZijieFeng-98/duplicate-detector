# ğŸ“… Daily Development Workflow

**Last Updated:** October 18, 2025  
**Quick Reference:** Your daily testing workflow

---

## ğŸš€ Quick Start (30 seconds)

```bash
# Before committing any code:
python test_pipeline_auto.py

# If all pass âœ… â†’ Safe to commit!
# If any fail âŒ â†’ Fix issues first
```

---

## ğŸ“‹ Complete Workflow

### **Step 1: Make Code Changes**
```bash
# Edit your code
nano ai_pdf_panel_duplicate_check_AUTO.py
# or
nano streamlit_app.py
```

### **Step 2: Run Quick Smoke Test (2-3 min)**
```bash
./quick_test.sh
```

**Expected Output:**
```
âœ… SUCCESS: Found 108 duplicate pairs
ğŸ“‹ Metadata: Runtime: 93.1s, Panels: 107
```

### **Step 3: Run Full Test Suite (8-10 min)**
```bash
python test_pipeline_auto.py
```

**What it checks:**
- âœ… Prerequisites (packages installed)
- âœ… sklearn import (deployment fix)
- âœ… Pipeline execution (2 configs)
- âœ… Output structure
- âœ… Pages & panels extraction
- âœ… Duplicate detection
- âœ… Tier classification
- âœ… Empty TSV prevention
- âœ… Performance benchmarks
- âœ… Visual quality

**Expected Output:**
```
ğŸ‰ ALL TESTS PASSED!
Tests run: 22
Passed: 57 âœ…
Failed: 0 âŒ
Time: 518.3s
```

### **Step 4: Check for Regressions**
```bash
# View test history
cat test_history.json | jq '.[-3:]'

# Check summary
cat test_output/test_summary.txt
```

**Look for:**
- âš ï¸ Performance regression warnings (>20% slower)
- âŒ Any test failures
- âœ… All green = safe to commit

### **Step 5: Commit & Push**
```bash
# Only if all tests pass!
git add .
git commit -m "Your descriptive message"
git push origin main
```

---

## ğŸ¯ Cursor Integration

### **Ask Cursor for Help:**

```
"Run tests"
â†’ Cursor executes: python test_pipeline_auto.py

"Check for regressions"
â†’ Cursor analyzes: test_history.json

"Why did test X fail?"
â†’ Cursor checks: test_output/*/test_run.log

"Show performance trends"
â†’ Cursor displays: Last 5 runs from test_history.json
```

### **Cursor Knows:**
- Performance baselines (90s, 300s)
- Regression thresholds (20%)
- Expected detection counts (107 panels, 108+ pairs)
- Test history for trend analysis

---

## ğŸ“Š Understanding Test Output

### **Green Status (All Tests Passed):**
```
======================================================================
ğŸ“Š TEST SUMMARY
======================================================================
  Tests run: 22
  Passed: 57 âœ…
  Failed: 0 âŒ
  Time: 518.3s
======================================================================
ğŸ‰ ALL TESTS PASSED!
```

**Action:** âœ… Safe to commit and deploy

---

### **Yellow Warnings (Non-Critical):**
```
======================================================================
ğŸ§ª TEST: Performance Benchmarks: Balanced_Default
======================================================================
  âš ï¸  Performance regression: 110.5s (expected: 90.0s, +22.8%)
```

**What it means:**
- Runtime increased >20% from baseline
- May indicate performance degradation
- Review recent code changes

**Action:** 
1. Check if intentional (e.g., added features)
2. Profile code for bottlenecks
3. Consider optimizations

---

### **Red Failures (Critical):**
```
======================================================================
ğŸ§ª TEST: sklearn Import (Deployment Fix)
======================================================================
  âŒ sklearn import failed: No module named 'sklearn'
```

**What it means:**
- Critical deployment fix broke
- Would fail in production

**Action:**
1. âŒ DO NOT COMMIT
2. Fix the issue immediately
3. Re-run tests
4. Only commit after âœ…

---

## ğŸ”§ Troubleshooting

### **Issue: Tests timeout**
```bash
# Increase timeout in test_pipeline_auto.py:
result = subprocess.run(cmd, timeout=1200)  # 20 min instead of 10
```

### **Issue: Tests fail on local PDF path**
```bash
# Update TEST_PDF_PATH in test_pipeline_auto.py:
TEST_PDF_PATH = Path("your/correct/path.pdf")
```

### **Issue: "No module named X"**
```bash
# Reinstall dependencies:
pip install -r requirements.txt
```

### **Issue: Cache affecting results**
```bash
# Clear cache and re-run:
rm -rf test_output/
python test_pipeline_auto.py
```

---

## ğŸ“ˆ Monitoring Performance Over Time

### **View Last 5 Test Runs:**
```bash
cat test_history.json | jq '.[-5:] | .[] | {
  date: .timestamp,
  runtime: .runtime_seconds,
  passed: .tests_passed,
  commit: .git_commit
}'
```

### **Check for Performance Trends:**
```bash
# Extract runtimes
cat test_history.json | jq '.[].runtime_seconds'

# Average of last 5 runs
cat test_history.json | jq '.[-5:] | map(.runtime_seconds) | add / length'
```

### **Identify Slowest Tests:**
```bash
# Check individual test logs
grep "Runtime:" test_output/*/RUN_METADATA.json
```

---

## ğŸ¯ Best Practices

### **âœ… DO:**

1. **Run tests before every commit**
   ```bash
   python test_pipeline_auto.py && git commit -m "..."
   ```

2. **Check test history after failures**
   ```bash
   cat test_history.json | jq '.[-3:]'
   ```

3. **Keep test data small**
   - Use 10-20 page test PDF
   - Avoid huge files (>100 pages)

4. **Update baselines when intentionally changing performance**
   ```python
   # In test_pipeline_auto.py:
   "expected_runtime": 100.0,  # Updated from 90.0
   ```

5. **Ask Cursor for help**
   ```
   @test_pipeline_auto.py Why did performance regress?
   ```

### **âŒ DON'T:**

1. **Skip tests before committing**
   - Could deploy broken code
   - Wastes time fixing production issues

2. **Ignore regression warnings**
   - Small slowdowns accumulate
   - Address early before they compound

3. **Commit with failing tests**
   - Breaks CI/CD pipeline
   - Blocks other developers

4. **Modify baselines without reason**
   - Defeats purpose of regression detection
   - Only update if intentional change

---

## ğŸ“š Quick Reference

### **Test Commands:**
```bash
# Quick (2-3 min)
./quick_test.sh

# Full (8-10 min)
python test_pipeline_auto.py

# With verbose output
python test_pipeline_auto.py -v
```

### **Check Results:**
```bash
# Summary
cat test_output/test_summary.txt

# History
cat test_history.json | jq '.'

# Last run
cat test_history.json | jq '.[-1]'

# Logs
cat test_output/*/test_run.log
```

### **Cursor Commands:**
```
"Run tests"
"Check for regressions"
"Show performance trends"
"Why did test X fail?"
"Add test for feature Y"
```

---

## ğŸŠ Success Checklist

Before pushing code, ensure:

- [ ] âœ… All 22 tests passed
- [ ] âœ… No performance regressions (< 20% slower)
- [ ] âœ… Visual outputs valid
- [ ] âœ… TSV files non-empty
- [ ] âœ… Test history updated
- [ ] âœ… Summary generated

If all checked â†’ **Safe to push!** ğŸš€

---

## ğŸ“– Related Documentation

- **TESTING_GUIDE.md** - Comprehensive testing guide
- **TESTING_COMPLETE.md** - Quick start
- **ENHANCED_TEST_REPORT.md** - Latest test results
- **TEST_REPORT_20251018.md** - Initial test results
- **.cursorrules** - Cursor AI integration

---

## ğŸ†˜ Need Help?

1. **Read test logs:**
   ```bash
   cat test_output/*/test_run.log | less
   ```

2. **Ask Cursor:**
   ```
   @test_pipeline_auto.py Help me debug test failure
   ```

3. **Check documentation:**
   - TESTING_GUIDE.md for detailed info
   - ENHANCED_TEST_REPORT.md for latest results

4. **Review recent changes:**
   ```bash
   git log --oneline -5
   git diff HEAD~1
   ```

---

**Last Updated:** October 18, 2025  
**Test Suite Version:** 2.0 (Enhanced)  
**Status:** ğŸŸ¢ Production Ready

---

## ğŸ‰ Remember

**Time savings per code change:** 39 minutes  
**Monthly savings:** ~13 hours  
**ROI:** < 2 days

**Your test suite saves you more time than it takes to run!** ğŸš€



