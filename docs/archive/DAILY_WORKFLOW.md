# 📅 Daily Development Workflow

**Last Updated:** October 18, 2025  
**Quick Reference:** Your daily testing workflow

---

## 🚀 Quick Start (30 seconds)

```bash
# Before committing any code:
python test_pipeline_auto.py

# If all pass ✅ → Safe to commit!
# If any fail ❌ → Fix issues first
```

---

## 📋 Complete Workflow

### **Step 1: Make Code Changes**
```bash
# Edit your code
nano ai_pdf_panel_duplicate_check_AUTO.py
# or
nano streamlit_app.py
```

### **Step 2: Run Quick Smoke Test (2-3 min)**
```bash
./quick_test.sh
```

**Expected Output:**
```
✅ SUCCESS: Found 108 duplicate pairs
📋 Metadata: Runtime: 93.1s, Panels: 107
```

### **Step 3: Run Full Test Suite (8-10 min)**
```bash
python test_pipeline_auto.py
```

**What it checks:**
- ✅ Prerequisites (packages installed)
- ✅ sklearn import (deployment fix)
- ✅ Pipeline execution (2 configs)
- ✅ Output structure
- ✅ Pages & panels extraction
- ✅ Duplicate detection
- ✅ Tier classification
- ✅ Empty TSV prevention
- ✅ Performance benchmarks
- ✅ Visual quality

**Expected Output:**
```
🎉 ALL TESTS PASSED!
Tests run: 22
Passed: 57 ✅
Failed: 0 ❌
Time: 518.3s
```

### **Step 4: Check for Regressions**
```bash
# View test history
cat test_history.json | jq '.[-3:]'

# Check summary
cat test_output/test_summary.txt
```

**Look for:**
- ⚠️ Performance regression warnings (>20% slower)
- ❌ Any test failures
- ✅ All green = safe to commit

### **Step 5: Commit & Push**
```bash
# Only if all tests pass!
git add .
git commit -m "Your descriptive message"
git push origin main
```

---

## 🎯 Cursor Integration

### **Ask Cursor for Help:**

```
"Run tests"
→ Cursor executes: python test_pipeline_auto.py

"Check for regressions"
→ Cursor analyzes: test_history.json

"Why did test X fail?"
→ Cursor checks: test_output/*/test_run.log

"Show performance trends"
→ Cursor displays: Last 5 runs from test_history.json
```

### **Cursor Knows:**
- Performance baselines (90s, 300s)
- Regression thresholds (20%)
- Expected detection counts (107 panels, 108+ pairs)
- Test history for trend analysis

---

## 📊 Understanding Test Output

### **Green Status (All Tests Passed):**
```
======================================================================
📊 TEST SUMMARY
======================================================================
  Tests run: 22
  Passed: 57 ✅
  Failed: 0 ❌
  Time: 518.3s
======================================================================
🎉 ALL TESTS PASSED!
```

**Action:** ✅ Safe to commit and deploy

---

### **Yellow Warnings (Non-Critical):**
```
======================================================================
🧪 TEST: Performance Benchmarks: Balanced_Default
======================================================================
  ⚠️  Performance regression: 110.5s (expected: 90.0s, +22.8%)
```

**What it means:**
- Runtime increased >20% from baseline
- May indicate performance degradation
- Review recent code changes

**Action:** 
1. Check if intentional (e.g., added features)
2. Profile code for bottlenecks
3. Consider optimizations

---

### **Red Failures (Critical):**
```
======================================================================
🧪 TEST: sklearn Import (Deployment Fix)
======================================================================
  ❌ sklearn import failed: No module named 'sklearn'
```

**What it means:**
- Critical deployment fix broke
- Would fail in production

**Action:**
1. ❌ DO NOT COMMIT
2. Fix the issue immediately
3. Re-run tests
4. Only commit after ✅

---

## 🔧 Troubleshooting

### **Issue: Tests timeout**
```bash
# Increase timeout in test_pipeline_auto.py:
result = subprocess.run(cmd, timeout=1200)  # 20 min instead of 10
```

### **Issue: Tests fail on local PDF path**
```bash
# Update TEST_PDF_PATH in test_pipeline_auto.py:
TEST_PDF_PATH = Path("your/correct/path.pdf")
```

### **Issue: "No module named X"**
```bash
# Reinstall dependencies:
pip install -r requirements.txt
```

### **Issue: Cache affecting results**
```bash
# Clear cache and re-run:
rm -rf test_output/
python test_pipeline_auto.py
```

---

## 📈 Monitoring Performance Over Time

### **View Last 5 Test Runs:**
```bash
cat test_history.json | jq '.[-5:] | .[] | {
  date: .timestamp,
  runtime: .runtime_seconds,
  passed: .tests_passed,
  commit: .git_commit
}'
```

### **Check for Performance Trends:**
```bash
# Extract runtimes
cat test_history.json | jq '.[].runtime_seconds'

# Average of last 5 runs
cat test_history.json | jq '.[-5:] | map(.runtime_seconds) | add / length'
```

### **Identify Slowest Tests:**
```bash
# Check individual test logs
grep "Runtime:" test_output/*/RUN_METADATA.json
```

---

## 🎯 Best Practices

### **✅ DO:**

1. **Run tests before every commit**
   ```bash
   python test_pipeline_auto.py && git commit -m "..."
   ```

2. **Check test history after failures**
   ```bash
   cat test_history.json | jq '.[-3:]'
   ```

3. **Keep test data small**
   - Use 10-20 page test PDF
   - Avoid huge files (>100 pages)

4. **Update baselines when intentionally changing performance**
   ```python
   # In test_pipeline_auto.py:
   "expected_runtime": 100.0,  # Updated from 90.0
   ```

5. **Ask Cursor for help**
   ```
   @test_pipeline_auto.py Why did performance regress?
   ```

### **❌ DON'T:**

1. **Skip tests before committing**
   - Could deploy broken code
   - Wastes time fixing production issues

2. **Ignore regression warnings**
   - Small slowdowns accumulate
   - Address early before they compound

3. **Commit with failing tests**
   - Breaks CI/CD pipeline
   - Blocks other developers

4. **Modify baselines without reason**
   - Defeats purpose of regression detection
   - Only update if intentional change

---

## 📚 Quick Reference

### **Test Commands:**
```bash
# Quick (2-3 min)
./quick_test.sh

# Full (8-10 min)
python test_pipeline_auto.py

# With verbose output
python test_pipeline_auto.py -v
```

### **Check Results:**
```bash
# Summary
cat test_output/test_summary.txt

# History
cat test_history.json | jq '.'

# Last run
cat test_history.json | jq '.[-1]'

# Logs
cat test_output/*/test_run.log
```

### **Cursor Commands:**
```
"Run tests"
"Check for regressions"
"Show performance trends"
"Why did test X fail?"
"Add test for feature Y"
```

---

## 🎊 Success Checklist

Before pushing code, ensure:

- [ ] ✅ All 22 tests passed
- [ ] ✅ No performance regressions (< 20% slower)
- [ ] ✅ Visual outputs valid
- [ ] ✅ TSV files non-empty
- [ ] ✅ Test history updated
- [ ] ✅ Summary generated

If all checked → **Safe to push!** 🚀

---

## 📖 Related Documentation

- **TESTING_GUIDE.md** - Comprehensive testing guide
- **TESTING_COMPLETE.md** - Quick start
- **ENHANCED_TEST_REPORT.md** - Latest test results
- **TEST_REPORT_20251018.md** - Initial test results
- **.cursorrules** - Cursor AI integration

---

## 🆘 Need Help?

1. **Read test logs:**
   ```bash
   cat test_output/*/test_run.log | less
   ```

2. **Ask Cursor:**
   ```
   @test_pipeline_auto.py Help me debug test failure
   ```

3. **Check documentation:**
   - TESTING_GUIDE.md for detailed info
   - ENHANCED_TEST_REPORT.md for latest results

4. **Review recent changes:**
   ```bash
   git log --oneline -5
   git diff HEAD~1
   ```

---

**Last Updated:** October 18, 2025  
**Test Suite Version:** 2.0 (Enhanced)  
**Status:** 🟢 Production Ready

---

## 🎉 Remember

**Time savings per code change:** 39 minutes  
**Monthly savings:** ~13 hours  
**ROI:** < 2 days

**Your test suite saves you more time than it takes to run!** 🚀

